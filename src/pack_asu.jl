"""
    pack_asu(points, N, T, ArrayType; shift=Tuple(zeros(length(N))))

Pack ASU points into dense blocks using a greedy rectangular decomposition.
"""
function pack_asu(points::Vector{ASUPoint}, N::Tuple, T::Type=Float64, ArrayType=Array; shift::NTuple=Tuple(zeros(Float64, length(N))))
    D = length(N)
    
    # 1. Group by Depth (blocks must share recursive depth/parity structure)
    depth_groups = Dict{Vector{Int}, Vector{ASUPoint}}()
    for p in points; push!(get!(depth_groups, p.depth, []), p); end
    
    blocks = ASUBlock{T, D, ArrayType{T,D}}[] # Preliminary type assumption (refined later)

    # 2. Greedy Packing per Group
    for (depth, group) in depth_groups
        sort!(group, by = p->p.idx)
        pool_indices = Set(p.idx for p in group)
        
        # Estimate strides: min non-zero diff per dimension
        strides = ones(Int, D)
        if length(group) > 1
            coords = hcat([p.idx for p in group]...)
            for d in 1:D
                vals = unique(sort(coords[d, :]))
                # Force stride 1 for Contiguous Blocks (Required by Modulated FFT Logic)
                strides[d] = 1
            end
        end

        pool = copy(group)
        while !isempty(pool)
            p_start = pool[1]
            start_idx = p_start.idx
            
            # Grow Box Heuristic
            current_shape = ones(Int, D)
            ranges = [start_idx[d]:strides[d]:start_idx[d] for d in 1:D] # Initial 1-point ranges
            
            for d in 1:D
                s = strides[d]
                steps = 0
                while true
                     # Check if next slice exists in pool
                     next_slice_idx = start_idx[d] + (steps+1)*s
                     check_ranges = copy(ranges)
                     check_ranges[d] = next_slice_idx:1:next_slice_idx # Check strict slice
                     
                     slice_valid = true
                     # Only check validity of points generated by extending CURRENT established face
                     # Refined logic: Iterating over the *face* we are extending.
                     # Dimensions < d have ranges established. Dims > d are single points.
                     
                     # Construct iterator for the check-face
                     check_iter_ranges = Any[ranges[k] for k in 1:D]
                     check_iter_ranges[d] = next_slice_idx:1:next_slice_idx
                     
                     if any(pt -> !(collect(pt) in pool_indices), Iterators.product(check_iter_ranges...))
                         break
                     end
                     steps += 1
                end
                
                ranges[d] = start_idx[d]:s:(start_idx[d] + steps*s)
            end
            
            # Create Block
            block_idx_iter = Iterators.product(ranges...)
            used = [collect(pt) for pt in block_idx_iter]
            
            dims = tuple(length.(ranges)...)
            data = ArrayType(zeros(T, dims)) # Allocate
            
            # Determine actual concrete type A for structure
            A_concrete = typeof(data)
            
            # Reconstruct correctly typed list if specific type needed?
            # We used generic list. Let's just push to a generic list and convert at end.
            push!(blocks, ASUBlock(data, convert(Vector{StepRange{Int,Int}}, ranges), depth))
            
            setdiff!(pool_indices, used)
            filter!(p -> p.idx in pool_indices, pool)
        end
    end

    # 3. Organize into Structure
    if isempty(blocks)
        A_final = ArrayType{T, D}
    else
        A_final = typeof(blocks[1].data)
    end
    
    dim_blocks = Dict{Int, Vector{ASUBlock{T, D, A_final}}}()
    for b in blocks
        # Re-wrap if type mismatch (e.g. initial list was generic)
        b_typed = ASUBlock(b.data, b.range, b.depth)
        eff_dim = count(r->length(r)>1, b.range)
        push!(get!(dim_blocks, eff_dim, []), b_typed)
    end
    
    return CrystallographicASU{D, T, A_final}(dim_blocks, shift)
end

"""
    pack_asu_interleaved(u::AbstractArray, N::Tuple, ops::Vector{SymOp}; 
                              L::Union{Nothing, Tuple}=nothing, asu_only::Bool=true)

Pack a full grid `u` into Interleaved ASUBlocks (Mode B).
If `asu_only=true` (default), only returns the single ASU subgrid (Γ₀).
If `asu_only=false`, returns all orbit representatives (legacy behavior).
Returns a `CrystallographicASU`.
"""
function pack_asu_interleaved(u::AbstractArray, N::Tuple, ops::Vector{SymOp}; 
                              L::Union{Nothing, Tuple}=nothing, asu_only::Bool=true)
    D = length(N)
    T = eltype(u)
    # Heuristic for ArrayType: Array or CuArray hint
    ArrayType = u isa Array ? Array : (u isa AbstractArray ? typeof(u).name.wrapper : Array)
    
    # 1. Analyze Orbits (Subgrid Discovery)
    # L defaults to (2,2,...) inside analyze_interleaved_orbits if nothing
    orbits = analyze_interleaved_orbits(N, ops; L=L)
    
    # Ensure L is set for slicing
    if isnothing(L)
        L = Tuple(fill(2, D))
    end
    
    # 2. ASU Selection: If asu_only, keep only the first orbit (Γ₀ = shift (0,0,...))
    if asu_only
        # Find the orbit with representative [0,0,0,...]
        asu_orbit = nothing
        for o in orbits
            if all(o.representative .== 0)
                asu_orbit = o
                break
            end
        end
        if isnothing(asu_orbit)
            # Fallback: use first orbit
            asu_orbit = first(orbits)
        end
        orbits = [asu_orbit]
    end
    
    blocks = ASUBlock{T, D, ArrayType{T,D}}[]
    
    for orbit in orbits
        # Representative shift: orbit.representative (0-based)
        # Subgrid Range: start:step:stop
        # start = orbit.representative[d] + 1 (1-based index)
        # step = L[d]
        
        rep_0based = orbit.representative
        range_tuples = Vector{StepRange{Int, Int}}(undef, D)
        
        for d in 1:D
            start = rep_0based[d] + 1
            step = L[d]
            # stop should be last valid index <= N[d]
            # Range logic handles stop automatically: start:step:N[d]
            range_tuples[d] = start : step : N[d]
        end
        
        # Slicing needs 1-based range (range_tuples)
        # ASUBlock needs Global 0-based range
        
        # Convert range_tuples (1-based) to 0-based
        current_range = Vector{StepRange{Int, Int}}(undef, D)
        for d in 1:D
             r = range_tuples[d]
             current_range[d] = (first(r)-1) : step(r) : (last(r)-1)
        end
        
        # Extract Data
        # If u is empty/dummy, we might skip allocation? No, ASUBlock needs data.
        # u[range_tuples...] creates a copy by default for StepRange slicing in standard Array.
        # For GPU arrays, it might be different.
        block_data = u[range_tuples...]
        
        # Ensure concrete ArrayType
        # If u is view, block_data is Array.
        # If ArrayType is specialized, convert.
        if !(block_data isa ArrayType{T, D})
            block_data = convert(ArrayType{T, D}, block_data)
        end
        
        # Depth: In Mode B, depth is irrelevant (use zeros)
        depth = zeros(Int, D)
        
        # For asu_only=true, use Mode A (no orbit info → correct reconstruction formula)
        # For asu_only=false, attach orbit info (Mode B orbit reduction)
        block_orbit = asu_only ? nothing : orbit
        push!(blocks, ASUBlock(block_data, current_range, depth, block_orbit))
    end
    
    # Organize into Structure
    dim_blocks = Dict{Int, Vector{ASUBlock{T, D, ArrayType{T,D}}}}()
    dim_blocks[D] = blocks # All blocks are full dimension
    
    return CrystallographicASU{D, T, ArrayType{T,D}}(dim_blocks, Tuple(zeros(Float64, D)))
end
